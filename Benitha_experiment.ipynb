{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Training Experiments for Bowling Atari Environment\n",
    "\n",
    "**Group Member Name:** Benitha Uwituze Rutagengwa\n",
    "\n",
    "This notebook loads the Bowling environment and runs **10 training experiments** with different hyperparameter configurations from the CSV file. Each experiment tests 10 different combinations of:\n",
    "- Learning Rate (lr)\n",
    "- Gamma (Î³) - Discount factor\n",
    "- Batch Size\n",
    "- Epsilon parameters (start, end, decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import ale_py to register ALE namespace\n",
    "import ale_py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Bowling environment...\n",
      "Environment loaded: <Monitor<OrderEnforcing<PassiveEnvChecker<AtariEnv<ALE/Bowling-v5>>>>>\n",
      "Action space: Discrete(6)\n",
      "Observation space: Box(0, 255, (210, 160, 3), uint8)\n",
      "\n",
      "Observation shape: (210, 160, 3)\n",
      "Environment is ready!\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_id=\"ALE/Bowling-v5\", render_mode=None):\n",
    "    \"\"\"Create and wrap the environment\"\"\"\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "# Test environment loading\n",
    "print(\"Loading Bowling environment...\")\n",
    "test_env = make_env()\n",
    "print(f\"Environment loaded: {test_env}\")\n",
    "print(f\"Action space: {test_env.action_space}\")\n",
    "print(f\"Observation space: {test_env.observation_space}\")\n",
    "\n",
    "# Test a random action\n",
    "obs, info = test_env.reset()\n",
    "print(f\"\\nObservation shape: {obs.shape}\")\n",
    "print(\"Environment is ready!\")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Hyperparameters from CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded hyperparameter configurations:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.99</th>\n",
       "      <th>32</th>\n",
       "      <th>1.1</th>\n",
       "      <th>0.05</th>\n",
       "      <th>1.00E-05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.95</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.97</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.90</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.99</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.95</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.92</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.96</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.93</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.99</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1   0.001  0.99   32  1.1  0.05  1.00E-05\n",
       "0   2  0.0005  0.95   64    1  0.10   0.00005\n",
       "1   3  0.0020  0.97   32    1  0.05   0.00002\n",
       "2   4  0.0008  0.90  128    1  0.05   0.00010\n",
       "3   5  0.0015  0.99   64    1  0.10   0.00010\n",
       "4   6  0.0003  0.95  128    1  0.05   0.00050\n",
       "5   7  0.0040  0.92   32    1  0.10   0.00002\n",
       "6   8  0.0012  0.96   64    1  0.05   0.00003\n",
       "7   9  0.0007  0.93   64    1  0.10   0.00001\n",
       "8  10  0.0030  0.99   32    1  0.05   0.00004"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepared 0 experiments for training\n",
      "\n",
      "WARNING: No experiments loaded! Check CSV file format.\n",
      "Expected columns: #, lr (Learning Rate), gamma (Discount Factor), batch_size, epsilon_start, epsilon_end, epsilon_decay\n"
     ]
    }
   ],
   "source": [
    "# Load hyperparameters from CSV\n",
    "csv_file = \"Benitha parameters for experiments.csv\"\n",
    "\n",
    "# Read CSV, skipping the first empty row\n",
    "df = pd.read_csv(csv_file, skiprows=1, skipinitialspace=True)\n",
    "\n",
    "# Clean column names (remove spaces and special characters)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Remove completely empty rows\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "# Display the loaded configurations\n",
    "print(\"Loaded hyperparameter configurations:\")\n",
    "print(\"=\"*80)\n",
    "display(df)\n",
    "\n",
    "# Prepare experiments list\n",
    "experiments = []\n",
    "for idx, row in df.iterrows():\n",
    "    # Check if this row has valid data\n",
    "    try:\n",
    "        # Get experiment number - handle both '#' and numeric index\n",
    "        exp_num = None\n",
    "        if '#' in row.index:\n",
    "            exp_num = row['#']\n",
    "        elif 0 in row.index:\n",
    "            exp_num = row.iloc[0]\n",
    "        \n",
    "        if pd.isna(exp_num) or exp_num == '':\n",
    "            continue\n",
    "        \n",
    "        # Try to convert to int to verify it's a number (not a header)\n",
    "        exp_num = int(float(exp_num))\n",
    "        \n",
    "        # Get learning rate\n",
    "        lr = None\n",
    "        if 'lr (Learning Rate)' in row.index:\n",
    "            lr = row['lr (Learning Rate)']\n",
    "        elif 'lr' in row.index:\n",
    "            lr = row['lr']\n",
    "        \n",
    "        if pd.isna(lr) or lr == '':\n",
    "            continue\n",
    "        \n",
    "        # Build experiment dictionary\n",
    "        exp = {\n",
    "            'num': exp_num,\n",
    "            'lr': float(row['lr (Learning Rate)']),\n",
    "            'gamma': float(row['gamma (Discount Factor)']),\n",
    "            'batch_size': int(row['batch_size']),\n",
    "            'epsilon_start': float(row['epsilon_start']),\n",
    "            'epsilon_end': float(row['epsilon_end']),\n",
    "            'epsilon_decay': float(row['epsilon_decay'])\n",
    "        }\n",
    "        experiments.append(exp)\n",
    "    except (ValueError, TypeError, KeyError) as e:\n",
    "        # Skip rows that can't be parsed (likely header or invalid rows)\n",
    "        print(f\"  Skipping row {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nPrepared {len(experiments)} experiments for training\")\n",
    "if len(experiments) > 0:\n",
    "    print(\"\\nExperiment configurations:\")\n",
    "    for exp in experiments:\n",
    "        print(f\"  Exp {exp['num']}: lr={exp['lr']}, gamma={exp['gamma']}, batch={exp['batch_size']}, \"\n",
    "              f\"eps={exp['epsilon_start']}->{exp['epsilon_end']}, decay={exp['epsilon_decay']}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No experiments loaded! Check CSV file format.\")\n",
    "    print(\"Expected columns: #, lr (Learning Rate), gamma (Discount Factor), batch_size, epsilon_start, epsilon_end, epsilon_decay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_epsilon_decay(epsilon_decay):\n",
    "    \"\"\"Convert epsilon_decay rate to exploration_fraction\"\"\"\n",
    "    if epsilon_decay < 0.0001:\n",
    "        return 0.8  # Very slow decay -> decay over most of training\n",
    "    elif epsilon_decay < 0.0005:\n",
    "        return 0.5  # Slow decay -> decay over middle portion\n",
    "    else:\n",
    "        return 0.2  # Fast decay -> decay over early portion\n",
    "\n",
    "def train_experiment(exp_config, total_timesteps=50000, policy_type=\"CnnPolicy\"):\n",
    "    \"\"\"\n",
    "    Train a DQN agent with specified hyperparameters\n",
    "    \n",
    "    Args:\n",
    "        exp_config: Dictionary with hyperparameters\n",
    "        total_timesteps: Total training timesteps\n",
    "        policy_type: \"CnnPolicy\" or \"MlpPolicy\"\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        log_dir: Path to logs\n",
    "        model_dir: Path to saved model\n",
    "        training_info: Dictionary with training statistics\n",
    "    \"\"\"\n",
    "    exp_num = exp_config['num']\n",
    "    \n",
    "    print(f\"EXPERIMENT {exp_num}\")\n",
    "    print(f\"Policy: {policy_type}\")\n",
    "    print(f\"Learning Rate: {exp_config['lr']}\")\n",
    "    print(f\"Gamma: {exp_config['gamma']}\")\n",
    "    print(f\"Batch Size: {exp_config['batch_size']}\")\n",
    "    print(f\"Epsilon: {exp_config['epsilon_start']} -> {exp_config['epsilon_end']}\")\n",
    "    \n",
    "    # Convert epsilon_decay to exploration_fraction\n",
    "    exploration_fraction = convert_epsilon_decay(exp_config['epsilon_decay'])\n",
    "    print(f\"Epsilon Decay Rate: {exp_config['epsilon_decay']} -> Exploration Fraction: {exploration_fraction}\")\n",
    "    print(f\"Total Timesteps: {total_timesteps}\")\n",
    "    \n",
    "    # Create directories\n",
    "    experiment_name = f\"benitha_exp_{exp_num}\"\n",
    "    log_dir = f\"benitha_logs/{experiment_name}_{policy_type}\"\n",
    "    model_dir = f\"benitha_models/{experiment_name}_{policy_type}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Create environments\n",
    "    env = DummyVecEnv([lambda: make_env()])\n",
    "    eval_env = DummyVecEnv([lambda: make_env()])\n",
    "    \n",
    "    # Initialize DQN agent\n",
    "    # Use smaller buffer_size for Atari to avoid memory issues (default is 1M which is too large)\n",
    "    model = DQN(\n",
    "        policy_type,\n",
    "        env,\n",
    "        learning_rate=exp_config['lr'],\n",
    "        gamma=exp_config['gamma'],\n",
    "        batch_size=exp_config['batch_size'],\n",
    "        buffer_size=5000,  # Reduced from default 1M to save memory (9.4GB instead of 94GB)\n",
    "        exploration_initial_eps=exp_config['epsilon_start'],\n",
    "        exploration_final_eps=exp_config['epsilon_end'],\n",
    "        exploration_fraction=exploration_fraction,\n",
    "        verbose=1,\n",
    "        tensorboard_log=log_dir,\n",
    "        device=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Setup callbacks\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=model_dir,\n",
    "        log_path=log_dir,\n",
    "        eval_freq=5000,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=10000,\n",
    "        save_path=model_dir,\n",
    "        name_prefix=\"dqn_checkpoint\"\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=[eval_callback, checkpoint_callback],\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = f\"{model_dir}/benitha_dqn_model.zip\"\n",
    "    model.save(final_model_path)\n",
    "    print(f\"\\nModel saved to: {final_model_path}\")\n",
    "    \n",
    "    # Extract training statistics from monitor\n",
    "    training_info = {\n",
    "        'experiment': exp_num,\n",
    "        'policy': policy_type,\n",
    "        'final_timesteps': total_timesteps\n",
    "    }\n",
    "    \n",
    "    # Try to get evaluation results\n",
    "    if hasattr(eval_callback, 'best_mean_reward'):\n",
    "        training_info['best_mean_reward'] = eval_callback.best_mean_reward\n",
    "        training_info['n_eval_episodes'] = eval_callback.n_eval_episodes\n",
    "    \n",
    "    return model, log_dir, model_dir, training_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Individual Experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 existing experiment results from CSV\n",
      "  - 7 successful experiments\n",
      "  - 0 failed/incomplete experiments\n",
      "\n",
      "Configuration:\n",
      "  Total timesteps per experiment: 30000\n",
      "  Policy type: CnnPolicy\n",
      "  Total experiments to run: 10\n",
      "  Estimated total time: ~300 minutes\n",
      "\n",
      "Note: Completed experiments will be skipped automatically.\n"
     ]
    }
   ],
   "source": [
    "# Configuration for all experiments\n",
    "TOTAL_TIMESTEPS = 30000  # Adjust based on available time\n",
    "POLICY_TYPE = \"CnnPolicy\"  # Use \"MlpPolicy\" for comparison\n",
    "\n",
    "# Load existing results if available (prevents re-running completed experiments)\n",
    "if os.path.exists('benitha_experiment_results.csv'):\n",
    "    try:\n",
    "        existing_df = pd.read_csv('benitha_experiment_results.csv')\n",
    "        all_results = existing_df.to_dict('records')\n",
    "        print(f\"Loaded {len(all_results)} existing experiment results from CSV\")\n",
    "        successful = [r for r in all_results if r.get('status') == 'success']\n",
    "        print(f\"  - {len(successful)} successful experiments\")\n",
    "        print(f\"  - {len(all_results) - len(successful)} failed/incomplete experiments\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load existing results: {e}\")\n",
    "        all_results = []\n",
    "        all_training_info = []\n",
    "else:\n",
    "    all_results = []\n",
    "    all_training_info = []\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Total timesteps per experiment: {TOTAL_TIMESTEPS}\")\n",
    "print(f\"  Policy type: {POLICY_TYPE}\")\n",
    "print(f\"  Total experiments to run: 10\")\n",
    "print(f\"  Estimated total time: ~{10 * TOTAL_TIMESTEPS / 1000:.0f} minutes\")\n",
    "print(f\"\\nNote: Completed experiments will be skipped automatically.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1 already completed. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: lr=0.0003, gamma=0.95, batch=32\n",
    "# Epsilon: 1.0 -> 0.01, decay=5e-05\n",
    "\n",
    "# Check if already completed\n",
    "exp_1_completed = any(r.get('experiment') == 1 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_1_completed:\n",
    "    print(\"Experiment 1 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 1,\n",
    "            'lr': 0.0010,\n",
    "            'gamma': 0.99,\n",
    "            'batch_size': 32,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.05,\n",
    "            'epsilon_decay': 1e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 1,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "        \n",
    "\n",
    "        print(f\"Experiment 1 completed in {duration:.2f} minutes\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        print(f\"  Model saved to: {model_dir}/benitha_dqn_model.zip\")\n",
    "        print(f\"  Results saved to: benitha_experiment_results.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 1: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 1,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 2 already completed. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Check if already completed\n",
    "exp_2_completed = any(r.get('experiment') == 2 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_2_completed:\n",
    "    print(\"Experiment 2 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 2,\n",
    "            'lr': 0.0005,\n",
    "            'gamma': 0.95,\n",
    "            'batch_size': 64,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.1,\n",
    "            'epsilon_decay': 5e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 2,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Experiment 2 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/benitha_dqn_model.zip\")\n",
    "        print(f\"  Results saved to: benitha_experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 2: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 2,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 3 already completed. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Check if already completed\n",
    "exp_3_completed = any(r.get('experiment') == 3 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_3_completed:\n",
    "    print(\"Experiment 3 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 3,\n",
    "            'lr': 0.0020,\n",
    "            'gamma': 0.97,\n",
    "            'batch_size': 32,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.05,\n",
    "            'epsilon_decay': 2e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 3,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Experiment 3 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/benitha_dqn_model.zip\")\n",
    "        print(f\"  Results saved to: benitha_experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 3: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 3,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 4 already completed. Skipping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if already completed\n",
    "exp_4_completed = any(r.get('experiment') == 4 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_4_completed:\n",
    "    print(\"Experiment 4 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 4,\n",
    "            'lr': 0.0008,\n",
    "            'gamma': 0.90,\n",
    "            'batch_size': 128,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.05,\n",
    "            'epsilon_decay': 1e-04\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 4,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Experiment 4 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/benitha_dqn_model.zip\")\n",
    "        print(f\"  Results saved to: benitha_experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 4: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 4,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 5 already completed. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Experiment 5: lr=0.0001, gamma=0.99, batch=64\n",
    "# Epsilon: 1.0 -> 0.05, decay=0.0001\n",
    "\n",
    "# Check if already completed\n",
    "exp_5_completed = any(r.get('experiment') == 5 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_5_completed:\n",
    "    print(\"Experiment 5 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 5,\n",
    "            'lr': 0.0015,\n",
    "            'gamma': 0.99,\n",
    "            'batch_size': 64,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.1,\n",
    "            'epsilon_decay': 1e-04\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 5,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Experiment 5 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/benitha_dqn_model.zip\")\n",
    "        print(f\"  Results saved to: benitha_experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 5: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 5,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 6 already completed. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Experiment 6: lr=0.0015, gamma=0.92, batch=128\n",
    "# Epsilon: 1.0 -> 0.1, decay=5e-05\n",
    "\n",
    "# Check if already completed\n",
    "exp_6_completed = any(r.get('experiment') == 6 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_6_completed:\n",
    "    print(\"Experiment 6 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 6,\n",
    "            'lr': 0.0003,\n",
    "            'gamma': 0.95,\n",
    "            'batch_size': 128,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.05,\n",
    "            'epsilon_decay': 5e-04\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 6,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Experiment 6 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/benitha_dqn_model.zip\")\n",
    "        print(f\"  Results saved to: benitha_experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 6: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 6,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 7 already completed. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Experiment 7: lr=0.0007, gamma=0.98, batch=32\n",
    "# Epsilon: 1.0 -> 0.01, decay=2e-05\n",
    "\n",
    "# Check if already completed\n",
    "exp_7_completed = any(r.get('experiment') == 7 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_7_completed:\n",
    "    print(\"Experiment 7 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 7,\n",
    "            'lr': 0.0040,\n",
    "            'gamma': 0.92,\n",
    "            'batch_size': 32,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.1,\n",
    "            'epsilon_decay': 2e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 7,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Experiment 7 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/benitha_dqn_model.zip\")\n",
    "        print(f\"  Results saved to: benitha_experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 7: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 7,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 8\n",
      "Policy: CnnPolicy\n",
      "Learning Rate: 0.0012\n",
      "Gamma: 0.96\n",
      "Batch Size: 64\n",
      "Epsilon: 1.0 -> 0.05\n",
      "Epsilon Decay Rate: 3e-05 -> Exploration Fraction: 0.8\n",
      "Total Timesteps: 30000\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Starting training...\n",
      "Logging to benitha_logs/benitha_exp_8_CnnPolicy\\DQN_2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 2002.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 2002.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2e+03    |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.802    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0012   |\n",
      "|    loss             | 3.66e-05 |\n",
      "|    n_updates        | 1224     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.11e+03 |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.666    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 6        |\n",
      "|    time_elapsed     | 1389     |\n",
      "|    total_timesteps  | 8435     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0012   |\n",
      "|    loss             | 5.93e-06 |\n",
      "|    n_updates        | 2083     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 27000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 27000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.7e+04  |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.604    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0012   |\n",
      "|    loss             | 8.36e-06 |\n",
      "|    n_updates        | 2474     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=15000, episode_reward=30.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=15000, episode_reward=30.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 2161.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 2161.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.16e+03 |\n",
      "|    mean_reward      | 30       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.406    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0012   |\n",
      "|    loss             | 3.85e-06 |\n",
      "|    n_updates        | 3724     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.11e+03 |\n",
      "|    ep_rew_mean      | 24       |\n",
      "|    exploration_rate | 0.331    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 5        |\n",
      "|    time_elapsed     | 3363     |\n",
      "|    total_timesteps  | 16912    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0012   |\n",
      "|    loss             | 3.84e-06 |\n",
      "|    n_updates        | 4202     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 27000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 27000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.7e+04  |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.208    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0012   |\n",
      "|    loss             | 9.07e-06 |\n",
      "|    n_updates        | 4974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=25000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=25000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 27000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 27000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.7e+04  |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0012   |\n",
      "|    loss             | 1.05e-05 |\n",
      "|    n_updates        | 6224     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.15e+03 |\n",
      "|    ep_rew_mean      | 26.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 4        |\n",
      "|    time_elapsed     | 5651     |\n",
      "|    total_timesteps  | 25778    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0012   |\n",
      "|    loss             | 0.00718  |\n",
      "|    n_updates        | 6419     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 27000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 27000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.7e+04  |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0012   |\n",
      "|    loss             | 8.31e-07 |\n",
      "|    n_updates        | 7474     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to: benitha_models/benitha_exp_8_CnnPolicy/benitha_dqn_model.zip\n",
      "Experiment 8 completed in 112.72 minutes\n",
      "  Model saved to: benitha_models/benitha_exp_8_CnnPolicy/benitha_dqn_model.zip\n",
      "  Results saved to: benitha_experiment_results.csv\n",
      "  Best Mean Reward: 30.00\n"
     ]
    }
   ],
   "source": [
    "# Experiment 8: lr=0.002, gamma=0.96, batch=64\n",
    "# Epsilon: 1.0 -> 0.05, decay=0.0001\n",
    "\n",
    "# Check if already completed\n",
    "exp_8_completed = any(r.get('experiment') == 8 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_8_completed:\n",
    "    print(\"Experiment 8 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 8,\n",
    "            'lr': 0.0012,\n",
    "            'gamma': 0.96,\n",
    "            'batch_size': 64,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.05,\n",
    "            'epsilon_decay': 3e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 8,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Experiment 8 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/benitha_dqn_model.zip\")\n",
    "        print(f\"  Results saved to: benitha_experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 8: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 8,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 9\n",
      "Policy: CnnPolicy\n",
      "Learning Rate: 0.0007\n",
      "Gamma: 0.93\n",
      "Batch Size: 64\n",
      "Epsilon: 1.0 -> 0.1\n",
      "Epsilon Decay Rate: 1e-05 -> Exploration Fraction: 0.8\n",
      "Total Timesteps: 30000\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Starting training...\n",
      "Logging to benitha_logs/benitha_exp_9_CnnPolicy\\DQN_1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 27000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 27000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.7e+04  |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.813    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 9.6e-06  |\n",
      "|    n_updates        | 1224     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.1e+03  |\n",
      "|    ep_rew_mean      | 25.2     |\n",
      "|    exploration_rate | 0.685    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 6        |\n",
      "|    time_elapsed     | 1339     |\n",
      "|    total_timesteps  | 8408     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 1.2e-05  |\n",
      "|    n_updates        | 2076     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=10000, episode_reward=30.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=10000, episode_reward=30.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 2161.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 2161.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.16e+03 |\n",
      "|    mean_reward      | 30       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.625    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 2.05e-05 |\n",
      "|    n_updates        | 2474     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=15000, episode_reward=30.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=15000, episode_reward=30.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 2161.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 2161.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.16e+03 |\n",
      "|    mean_reward      | 30       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.438    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 9.8e-06  |\n",
      "|    n_updates        | 3724     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.11e+03 |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.366    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 8        |\n",
      "|    time_elapsed     | 2084     |\n",
      "|    total_timesteps  | 16904    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 1.63e-05 |\n",
      "|    n_updates        | 4200     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 2002.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 2002.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2e+03    |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.25     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 9.89e-06 |\n",
      "|    n_updates        | 4974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=25000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=25000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 27000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 27000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.7e+04  |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 5.42e-06 |\n",
      "|    n_updates        | 6224     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.13e+03 |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 7        |\n",
      "|    time_elapsed     | 3584     |\n",
      "|    total_timesteps  | 25599    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 1.69e-05 |\n",
      "|    n_updates        | 6374     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 27000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 27000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.7e+04  |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 0.00734  |\n",
      "|    n_updates        | 7474     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to: benitha_models/benitha_exp_9_CnnPolicy/benitha_dqn_model.zip\n",
      "Experiment 9 completed in 78.38 minutes\n",
      "  Model saved to: benitha_models/benitha_exp_9_CnnPolicy/benitha_dqn_model.zip\n",
      "  Results saved to: benitha_experiment_results.csv\n",
      "  Best Mean Reward: 30.00\n"
     ]
    }
   ],
   "source": [
    "# Experiment 9: lr=0.0004, gamma=0.93, batch=128\n",
    "# Epsilon: 1.0 -> 0.1, decay=0.0005\n",
    "\n",
    "# Check if already completed\n",
    "exp_9_completed = any(r.get('experiment') == 9 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_9_completed:\n",
    "    print(\"Experiment 9 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 9,\n",
    "            'lr': 0.0007,\n",
    "            'gamma': 0.93,\n",
    "            'batch_size': 64,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.1,\n",
    "            'epsilon_decay': 1e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 9,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Experiment 9 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/benitha_dqn_model.zip\")\n",
    "        print(f\"  Results saved to: benitha_experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 9: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 9,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 10\n",
      "Policy: CnnPolicy\n",
      "Learning Rate: 0.003\n",
      "Gamma: 0.99\n",
      "Batch Size: 32\n",
      "Epsilon: 1.0 -> 0.05\n",
      "Epsilon Decay Rate: 4e-05 -> Exploration Fraction: 0.8\n",
      "Total Timesteps: 30000\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Starting training...\n",
      "Logging to benitha_logs/benitha_exp_10_CnnPolicy\\DQN_1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 27000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 27000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.7e+04  |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.802    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.003    |\n",
      "|    loss             | 1.16e-05 |\n",
      "|    n_updates        | 1224     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.11e+03 |\n",
      "|    ep_rew_mean      | 18       |\n",
      "|    exploration_rate | 0.666    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 7        |\n",
      "|    time_elapsed     | 1186     |\n",
      "|    total_timesteps  | 8429     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.003    |\n",
      "|    loss             | 5.46e-06 |\n",
      "|    n_updates        | 2082     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=10000, episode_reward=30.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=10000, episode_reward=30.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 2161.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 2161.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.16e+03 |\n",
      "|    mean_reward      | 30       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.604    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.003    |\n",
      "|    loss             | 5.03e-05 |\n",
      "|    n_updates        | 2474     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=15000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=15000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 27000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 27000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.7e+04  |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.406    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.003    |\n",
      "|    loss             | 6.91e-06 |\n",
      "|    n_updates        | 3724     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.12e+03 |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.328    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 6        |\n",
      "|    time_elapsed     | 2483     |\n",
      "|    total_timesteps  | 16974    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.003    |\n",
      "|    loss             | 1.61e-05 |\n",
      "|    n_updates        | 4218     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 27000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 27000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.7e+04  |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.208    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.003    |\n",
      "|    loss             | 1.84e-05 |\n",
      "|    n_updates        | 4974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=25000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=25000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 27000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 27000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2.7e+04  |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.003    |\n",
      "|    loss             | 1.33e-05 |\n",
      "|    n_updates        | 6224     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 5        |\n",
      "|    time_elapsed     | 4974     |\n",
      "|    total_timesteps  | 25723    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.003    |\n",
      "|    loss             | 0.0779   |\n",
      "|    n_updates        | 6405     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 2002.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 2002.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 2e+03    |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.003    |\n",
      "|    loss             | 0.0761   |\n",
      "|    n_updates        | 7474     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to: benitha_models/benitha_exp_10_CnnPolicy/benitha_dqn_model.zip\n",
      "Experiment 10 completed in 93.75 minutes\n",
      "  Model saved to: benitha_models/benitha_exp_10_CnnPolicy/benitha_dqn_model.zip\n",
      "  Results saved to: benitha_experiment_results.csv\n",
      "  Best Mean Reward: 30.00\n"
     ]
    }
   ],
   "source": [
    "# Experiment 10: lr=0.0035, gamma=0.99, batch=32\n",
    "# Epsilon: 1.0 -> 0.02, decay=3e-05\n",
    "\n",
    "# Check if already completed\n",
    "exp_10_completed = any(r.get('experiment') == 10 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_10_completed:\n",
    "    print(\"Experiment 10 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 10,\n",
    "            'lr': 0.0030,\n",
    "            'gamma': 0.99,\n",
    "            'batch_size': 32,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.05,\n",
    "            'epsilon_decay': 4e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 10,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Experiment 10 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/benitha_dqn_model.zip\")\n",
    "        print(f\"  Results saved to: benitha_experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 10: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 10,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Experiments Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENTS SUMMARY\n",
      "\n",
      "Completed: 10/10\n",
      "Failed: 0/10\n",
      "\n",
      "Successful experiments: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "Reward Statistics:\n",
      "  Best: 35.40\n",
      "  Average: 21.34\n",
      "\n",
      "Results saved to benitha_experiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Summary of all completed experiments\n",
    "# Load results from CSV if available\n",
    "if os.path.exists('benitha_experiment_results.csv'):\n",
    "    try:\n",
    "        results_df = pd.read_csv('benitha_experiment_results.csv')\n",
    "        all_results = results_df.to_dict('records')\n",
    "    except:\n",
    "        all_results = []\n",
    "elif 'all_results' in globals():\n",
    "    # Use in-memory results\n",
    "    pass\n",
    "else:\n",
    "    all_results = []\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    successful = [r for r in all_results if r.get('status') == 'success']\n",
    "    failed = [r for r in all_results if r.get('status') == 'failed']\n",
    "    \n",
    "    print(f\"EXPERIMENTS SUMMARY\")\n",
    "    print(f\"\\nCompleted: {len(successful)}/10\")\n",
    "    print(f\"Failed: {len(failed)}/10\")\n",
    "    \n",
    "    if len(successful) > 0:\n",
    "        print(f\"\\nSuccessful experiments: {sorted([r['experiment'] for r in successful])}\")\n",
    "        # Show rewards if available\n",
    "        rewards = [r.get('best_mean_reward') for r in successful if pd.notna(r.get('best_mean_reward'))]\n",
    "        if rewards:\n",
    "            print(f\"\\nReward Statistics:\")\n",
    "            print(f\"  Best: {max(rewards):.2f}\")\n",
    "            print(f\"  Average: {sum(rewards)/len(rewards):.2f}\")\n",
    "    if len(failed) > 0:\n",
    "        print(f\"\\nFailed experiments: {sorted([r['experiment'] for r in failed])}\")\n",
    "    \n",
    "    # Save results to CSV for persistence\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "    print(f\"\\nResults saved to benitha_experiment_results.csv\")\n",
    "else:\n",
    "    print(\"No experiments completed yet. Run the experiment cells above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare MLPPolicy vs CNNPolicy (Required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare policies using the first experiment configuration\n",
    "# This is REQUIRED for the assignment - compare MLPPolicy vs CNNPolicy\n",
    "if len(experiments) > 0:\n",
    "    comparison_exp = experiments[0].copy()\n",
    "    \n",
    "    print(\"COMPARING MLPPolicy vs CNNPolicy\")\n",
    "    print(f\"Using experiment {comparison_exp['num']} configuration\")\n",
    "    print(\"This comparison is required for the assignment.\")\n",
    "    \n",
    "    comparison_results = {}\n",
    "    \n",
    "    for policy in [\"MlpPolicy\", \"CnnPolicy\"]:\n",
    "        print(f\"Training with {policy}\")\n",
    "        \n",
    "        try:\n",
    "            model, log_dir, model_dir, training_info = train_experiment(\n",
    "                comparison_exp,\n",
    "                total_timesteps=20000,  # Reduced for comparison\n",
    "                policy_type=policy\n",
    "            )\n",
    "            \n",
    "            comparison_results[policy] = {\n",
    "                'status': 'success',\n",
    "                'log_dir': log_dir,\n",
    "                'model_dir': model_dir,\n",
    "                'best_mean_reward': training_info.get('best_mean_reward', None)\n",
    "            }\n",
    "            \n",
    "            if training_info.get('best_mean_reward') is not None:\n",
    "                print(f\"\\n{policy} Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {policy}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            comparison_results[policy] = {\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    print(\"POLICY COMPARISON COMPLETE\")\n",
    "    print(\"\\nResults Summary:\")\n",
    "    for policy, result in comparison_results.items():\n",
    "        if result['status'] == 'success':\n",
    "            reward_info = f\"Best Reward: {result.get('best_mean_reward', 'N/A')}\" if result.get('best_mean_reward') else \"Reward: N/A\"\n",
    "            print(f\"  {policy}: {reward_info}\")\n",
    "            print(f\"    Logs: {result['log_dir']}\")\n",
    "    print(\"\\nView TensorBoard logs to compare performance:\")\n",
    "    print(\"  tensorboard --logdir benitha_logs/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT RESULTS SUMMARY\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>status</th>\n",
       "      <th>lr</th>\n",
       "      <th>gamma</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epsilon_start</th>\n",
       "      <th>epsilon_end</th>\n",
       "      <th>epsilon_decay</th>\n",
       "      <th>best_mean_reward</th>\n",
       "      <th>duration_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>success</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.99</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.809031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>success</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.95</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>35.4</td>\n",
       "      <td>129.450441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>success</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.97</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.039755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>success</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.90</td>\n",
       "      <td>128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.919371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>success</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.99</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.100118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>success</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.95</td>\n",
       "      <td>128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>28.0</td>\n",
       "      <td>120.271615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>success</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.92</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>30.0</td>\n",
       "      <td>105.477129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>success</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.96</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>30.0</td>\n",
       "      <td>112.718469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>success</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.93</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>30.0</td>\n",
       "      <td>78.381118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>success</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.99</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>30.0</td>\n",
       "      <td>93.752422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment   status      lr  gamma  batch_size  epsilon_start  epsilon_end  \\\n",
       "0           1  success  0.0010   0.99          32            1.0         0.05   \n",
       "1           2  success  0.0005   0.95          64            1.0         0.10   \n",
       "2           3  success  0.0020   0.97          32            1.0         0.05   \n",
       "3           4  success  0.0008   0.90         128            1.0         0.05   \n",
       "4           5  success  0.0015   0.99          64            1.0         0.10   \n",
       "5           6  success  0.0003   0.95         128            1.0         0.05   \n",
       "6           7  success  0.0040   0.92          32            1.0         0.10   \n",
       "7           8  success  0.0012   0.96          64            1.0         0.05   \n",
       "8           9  success  0.0007   0.93          64            1.0         0.10   \n",
       "9          10  success  0.0030   0.99          32            1.0         0.05   \n",
       "\n",
       "   epsilon_decay  best_mean_reward  duration_minutes  \n",
       "0        0.00001              30.0         80.809031  \n",
       "1        0.00005              35.4        129.450441  \n",
       "2        0.00002               0.0         94.039755  \n",
       "3        0.00010               0.0         89.919371  \n",
       "4        0.00010               0.0        132.100118  \n",
       "5        0.00050              28.0        120.271615  \n",
       "6        0.00002              30.0        105.477129  \n",
       "7        0.00003              30.0        112.718469  \n",
       "8        0.00001              30.0         78.381118  \n",
       "9        0.00004              30.0         93.752422  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY STATISTICS\n",
      "Successful experiments: 10\n",
      "Average training time: 103.69 minutes\n",
      "Total training time: 1036.92 minutes\n",
      "\n",
      "Reward Statistics:\n",
      "  Best reward: 35.40\n",
      "  Worst reward: 0.00\n",
      "  Average reward: 21.34\n",
      "  Median reward: 30.00\n",
      "\n",
      " Results saved to benitha_experiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "    \n",
    "    # Display key columns\n",
    "    display_cols = ['experiment', 'status', 'lr', 'gamma', 'batch_size', \n",
    "                    'epsilon_start', 'epsilon_end', 'epsilon_decay', \n",
    "                    'best_mean_reward', 'duration_minutes']\n",
    "    available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "    display(results_df[available_cols])\n",
    "    \n",
    "    # Summary statistics\n",
    "    successful = results_df[results_df['status'] == 'success']\n",
    "    if len(successful) > 0:\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(f\"Successful experiments: {len(successful)}\")\n",
    "        print(f\"Average training time: {successful['duration_minutes'].mean():.2f} minutes\")\n",
    "        print(f\"Total training time: {successful['duration_minutes'].sum():.2f} minutes\")\n",
    "        \n",
    "        if 'best_mean_reward' in successful.columns:\n",
    "            valid_rewards = successful['best_mean_reward'].dropna()\n",
    "            if len(valid_rewards) > 0:\n",
    "                print(f\"\\nReward Statistics:\")\n",
    "                print(f\"  Best reward: {valid_rewards.max():.2f}\")\n",
    "                print(f\"  Worst reward: {valid_rewards.min():.2f}\")\n",
    "                print(f\"  Average reward: {valid_rewards.mean():.2f}\")\n",
    "                print(f\"  Median reward: {valid_rewards.median():.2f}\")\n",
    "    \n",
    "    failed = results_df[results_df['status'] == 'failed']\n",
    "    if len(failed) > 0:\n",
    "        print(f\"\\nFailed experiments: {len(failed)}\")\n",
    "        for idx, row in failed.iterrows():\n",
    "            print(f\"  Experiment {row['experiment']}: {row.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv('benitha_experiment_results.csv', index=False)\n",
    "    print(\"\\n Results saved to benitha_experiment_results.csv\")\n",
    "else:\n",
    "    print(\"No results to display. Run experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating hyperparameter table with behavior analysis...\n",
      "\n",
      "========================================================================================================================\n",
      "HYPERPARAMETER TUNING TABLE - COPY TO README.md\n",
      "(Observed Behaviors generated from training results)\n",
      "========================================================================================================================\n",
      "\n",
      "| Experiment # | Learning Rate | Gamma | Batch Size | Epsilon Start | Epsilon End | Epsilon Decay | Observed Behavior |\n",
      "|--------------|---------------|-------|------------|---------------|-------------|---------------|-------------------|\n",
      "| 1 | 0.001 | 0.99 | 32 | 1.0 | 0.05 | 1e-05 | Excellent performance (top 10%). Best reward: 30.00. Moderate LR - balanced learning. High gamma ... |\n",
      "| 2 | 0.0005 | 0.95 | 64 | 1.0 | 0.1 | 5e-05 | Excellent performance (top 0%). Best reward: 35.40. Moderate LR - balanced learning. Moderate gam... |\n",
      "| 3 | 0.002 | 0.97 | 32 | 1.0 | 0.05 | 2e-05 | Below average performance. Best reward: 0.00. Moderate LR - balanced learning. Moderate gamma - b... |\n",
      "| 4 | 0.0008 | 0.9 | 128 | 1.0 | 0.05 | 0.0001 | Below average performance. Best reward: 0.00. Moderate LR - balanced learning. Low gamma - short-... |\n",
      "| 5 | 0.0015 | 0.99 | 64 | 1.0 | 0.1 | 0.0001 | Below average performance. Best reward: 0.00. Moderate LR - balanced learning. High gamma - long-... |\n",
      "| 6 | 0.0003 | 0.95 | 128 | 1.0 | 0.05 | 0.0005 | Moderate performance (average). Best reward: 28.00. Moderate LR - balanced learning. Moderate gam... |\n",
      "| 7 | 0.004 | 0.92 | 32 | 1.0 | 0.1 | 2e-05 | Excellent performance (top 10%). Best reward: 30.00. High LR - may cause instability. Low gamma -... |\n",
      "| 8 | 0.0012 | 0.96 | 64 | 1.0 | 0.05 | 3e-05 | Excellent performance (top 10%). Best reward: 30.00. Moderate LR - balanced learning. Moderate ga... |\n",
      "| 9 | 0.0007 | 0.93 | 64 | 1.0 | 0.1 | 1e-05 | Excellent performance (top 10%). Best reward: 30.00. Moderate LR - balanced learning. Moderate ga... |\n",
      "| 10 | 0.003 | 0.99 | 32 | 1.0 | 0.05 | 4e-05 | Excellent performance (top 10%). Best reward: 30.00. High LR - may cause instability. High gamma ... |\n",
      "\n",
      "NOTE: Behaviors are generated from training metrics.\n",
      "You can enhance these descriptions by:\n",
      "1. Reviewing TensorBoard logs for detailed reward trends\n",
      "2. Observing episode lengths and convergence patterns\n",
      "3. Comparing with other experiments\n",
      "4. Adding specific observations about stability and learning speed\n",
      "DETAILED EXPERIMENT ANALYSIS\n",
      "\n",
      "Best Performing Experiment: 2\n",
      "  Reward: 35.40\n",
      "\n",
      "Worst Performing Experiment: 3\n",
      "  Reward: 0.00\n",
      "\n",
      "Average Reward Across All Experiments: 21.34\n",
      "Standard Deviation: 14.84\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze and generate observed behaviors\n",
    "def analyze_experiment_behavior(row, all_successful):\n",
    "    \"\"\"\n",
    "    Automatically analyze experiment results and generate behavior description\n",
    "    Args:\n",
    "        row: Single experiment row from results DataFrame\n",
    "        all_successful: All successful experiments for comparison\n",
    "    Returns:\n",
    "        behavior_description: String describing observed behavior\n",
    "    \"\"\"\n",
    "    behaviors = []\n",
    "    \n",
    "    # Get reward information\n",
    "    best_reward = row.get('best_mean_reward')\n",
    "    if pd.notna(best_reward):\n",
    "        # Compare with other experiments\n",
    "        all_rewards = all_successful['best_mean_reward'].dropna()\n",
    "        if len(all_rewards) > 0:\n",
    "            reward_percentile = (all_rewards <= best_reward).sum() / len(all_rewards) * 100\n",
    "            if reward_percentile >= 80:\n",
    "                behaviors.append(f\"Excellent performance (top {100-int(reward_percentile)}%)\")\n",
    "            elif reward_percentile >= 60:\n",
    "                behaviors.append(f\"Good performance (above average)\")\n",
    "            elif reward_percentile >= 40:\n",
    "                behaviors.append(f\"Moderate performance (average)\")\n",
    "            else:\n",
    "                behaviors.append(f\"Below average performance\")\n",
    "            behaviors.append(f\"Best reward: {best_reward:.2f}\")\n",
    "        else:\n",
    "            behaviors.append(f\"Best reward: {best_reward:.2f}\")\n",
    "    \n",
    "    # Analyze learning rate\n",
    "    lr = row['lr']\n",
    "    if lr > 0.002:\n",
    "        behaviors.append(\"High LR - may cause instability\")\n",
    "    elif lr < 0.0002:\n",
    "        behaviors.append(\"Low LR - slower but stable learning\")\n",
    "    else:\n",
    "        behaviors.append(\"Moderate LR - balanced learning\")\n",
    "    \n",
    "    # Analyze gamma (discount factor)\n",
    "    gamma = row['gamma']\n",
    "    if gamma >= 0.98:\n",
    "        behaviors.append(\"High gamma - long-term focus\")\n",
    "    elif gamma <= 0.92:\n",
    "        behaviors.append(\"Low gamma - short-term focus\")\n",
    "    else:\n",
    "        behaviors.append(\"Moderate gamma - balanced\")\n",
    "    \n",
    "    # Analyze batch size\n",
    "    batch = row['batch_size']\n",
    "    if batch >= 128:\n",
    "        behaviors.append(\"Large batch - stable gradients\")\n",
    "    elif batch <= 32:\n",
    "        behaviors.append(\"Small batch - faster updates\")\n",
    "    else:\n",
    "        behaviors.append(\"Medium batch - balanced\")\n",
    "    \n",
    "    # Analyze epsilon decay\n",
    "    eps_decay = row['epsilon_decay']\n",
    "    if eps_decay < 0.0001:\n",
    "        behaviors.append(\"Slow exploration decay - more exploration\")\n",
    "    elif eps_decay > 0.0003:\n",
    "        behaviors.append(\"Fast exploration decay - quick exploitation\")\n",
    "    else:\n",
    "        behaviors.append(\"Moderate exploration decay\")\n",
    "    \n",
    "    # Analyze training duration (if available)\n",
    "    if 'duration_minutes' in row and pd.notna(row['duration_minutes']):\n",
    "        duration = row['duration_minutes']\n",
    "        avg_duration = all_successful['duration_minutes'].mean()\n",
    "        if duration < avg_duration * 0.9:\n",
    "            behaviors.append(\"Faster training\")\n",
    "        elif duration > avg_duration * 1.1:\n",
    "            behaviors.append(\"Slower training\")\n",
    "    \n",
    "    # Combine all behaviors\n",
    "    if behaviors:\n",
    "        return \". \".join(behaviors) + \".\"\n",
    "    else:\n",
    "        return \"Training completed. Review TensorBoard for detailed metrics.\"\n",
    "\n",
    "# Generate hyperparameter table for documentation\n",
    "def create_hyperparameter_table(results_df, auto_fill=True):\n",
    "    \"\"\"Create formatted table for README documentation with behavior analysis\"\"\"\n",
    "    if results_df is None or len(results_df) == 0:\n",
    "        print(\"No results available. Run experiments first.\")\n",
    "        return\n",
    "    \n",
    "    successful = results_df[results_df['status'] == 'success'].copy()\n",
    "    \n",
    "    if len(successful) == 0:\n",
    "        print(\"No successful experiments to document.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*120)\n",
    "    print(\"HYPERPARAMETER TUNING TABLE - COPY TO README.md\")\n",
    "    if auto_fill:\n",
    "        print(\"(Observed Behaviors generated from training results)\")\n",
    "    print(\"=\"*120)\n",
    "    print(\"\\n| Experiment # | Learning Rate | Gamma | Batch Size | Epsilon Start | Epsilon End | Epsilon Decay | Observed Behavior |\")\n",
    "    print(\"|--------------|---------------|-------|------------|---------------|-------------|---------------|-------------------|\")\n",
    "    \n",
    "    for idx, row in successful.iterrows():\n",
    "        exp_num = int(row['experiment'])\n",
    "        lr = row['lr']\n",
    "        gamma = row['gamma']\n",
    "        batch = int(row['batch_size'])\n",
    "        eps_start = row['epsilon_start']\n",
    "        eps_end = row['epsilon_end']\n",
    "        eps_decay = row['epsilon_decay']\n",
    "        \n",
    "        # Generate automatic behavior description\n",
    "        if auto_fill:\n",
    "            behavior = analyze_experiment_behavior(row, successful)\n",
    "        else:\n",
    "            # Get reward info if available\n",
    "            reward_info = \"\"\n",
    "            if pd.notna(row.get('best_mean_reward')):\n",
    "                reward_info = f\"Best reward: {row['best_mean_reward']:.2f}. \"\n",
    "            behavior = f\"{reward_info}[DESCRIBE: Convergence speed, stability, final performance, reward trends, episode lengths]\"\n",
    "        \n",
    "        # Truncate behavior if too long for table\n",
    "        if len(behavior) > 100:\n",
    "            behavior = behavior[:97] + \"...\"\n",
    "        \n",
    "        print(f\"| {exp_num} | {lr} | {gamma} | {batch} | {eps_start} | {eps_end} | {eps_decay} | {behavior} |\")\n",
    "    \n",
    "    if auto_fill:\n",
    "        print(\"\\nNOTE: Behaviors are generated from training metrics.\")\n",
    "        print(\"You can enhance these descriptions by:\")\n",
    "        print(\"1. Reviewing TensorBoard logs for detailed reward trends\")\n",
    "        print(\"2. Observing episode lengths and convergence patterns\")\n",
    "        print(\"3. Comparing with other experiments\")\n",
    "        print(\"4. Adding specific observations about stability and learning speed\")\n",
    "    else:\n",
    "        print(\"\\nINSTRUCTIONS:\")\n",
    "        print(\"1. Review TensorBoard logs for each experiment\")\n",
    "        print(\"2. Note the reward trends (increasing, stable, fluctuating)\")\n",
    "        print(\"3. Observe episode lengths over time\")\n",
    "        print(\"4. Check convergence speed (how quickly rewards improve)\")\n",
    "        print(\"5. Note any stability issues (erratic behavior)\")\n",
    "        print(\"6. Record final performance metrics\")\n",
    "        print(\"7. Fill in the 'Observed Behavior' column with your findings\")\n",
    "        print(\"8. Copy this table to your README.md file\")\n",
    "\n",
    "# Generate the table with behavior analysis\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\nGenerating hyperparameter table with behavior analysis...\\n\")\n",
    "    create_hyperparameter_table(results_df, auto_fill=True)\n",
    "    \n",
    "    # Also save detailed analysis to a file\n",
    "    print(\"DETAILED EXPERIMENT ANALYSIS\")\n",
    "    successful = results_df[results_df['status'] == 'success'].copy()\n",
    "    if len(successful) > 0 and 'best_mean_reward' in successful.columns:\n",
    "        valid_rewards = successful['best_mean_reward'].dropna()\n",
    "        if len(valid_rewards) > 0:\n",
    "            print(f\"\\nBest Performing Experiment: {successful.loc[valid_rewards.idxmax(), 'experiment']}\")\n",
    "            print(f\"  Reward: {valid_rewards.max():.2f}\")\n",
    "            print(f\"\\nWorst Performing Experiment: {successful.loc[valid_rewards.idxmin(), 'experiment']}\")\n",
    "            print(f\"  Reward: {valid_rewards.min():.2f}\")\n",
    "            print(f\"\\nAverage Reward Across All Experiments: {valid_rewards.mean():.2f}\")\n",
    "            print(f\"Standard Deviation: {valid_rewards.std():.2f}\")\n",
    "else:\n",
    "    print(\"Run experiments first to generate the table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To view training progress with TensorBoard, run in terminal:\n",
      "\n",
      " tensorboard --logdir benitha_logs/\n",
      "\n",
      "Then open http://localhost:6006 in your browser\n",
      "\n",
      "Or use TensorBoard in Jupyter:\n",
      " %load_ext tensorboard\n",
      " %tensorboard --logdir benitha_logs/\n"
     ]
    }
   ],
   "source": [
    "# Instructions for viewing TensorBoard\n",
    "print(\"To view training progress with TensorBoard, run in terminal:\")\n",
    "print(\"\\n tensorboard --logdir benitha_logs/\")\n",
    "print(\"\\nThen open http://localhost:6006 in your browser\")\n",
    "print(\"\\nOr use TensorBoard in Jupyter:\")\n",
    "print(\" %load_ext tensorboard\")\n",
    "print(\" %tensorboard --logdir benitha_logs/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load and Test Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model and test it\n",
    "def test_model(model_path, num_episodes=3, render=False):\n",
    "    \"\"\"Test a trained model\"\"\"\n",
    "    \n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = make_env(render_mode=\"human\" if render else None)\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model = DQN.load(model_path, env=env)\n",
    "        print(\" Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading model: {e}\")\n",
    "        env.close()\n",
    "        return\n",
    "    \n",
    "    # Set to greedy policy (no exploration)\n",
    "    model.exploration_rate = 0.0\n",
    "    \n",
    "    # Test episodes\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"\\nAverage reward over {num_episodes} episodes: {avg_reward:.2f}\")\n",
    "    \n",
    "    return avg_reward\n",
    "\n",
    "# Test the best model from first experiment (if available)\n",
    "if len(experiments) > 0:\n",
    "    test_model_path = f\"benitha_models/benitha_exp_{experiments[0]['num']}_CnnPolicy/best_model.zip\"\n",
    "    if os.path.exists(test_model_path):\n",
    "        print(\"Testing trained model...\")\n",
    "        test_model(test_model_path, num_episodes=3, render=False)\n",
    "    else:\n",
    "        print(f\"Model not found at {test_model_path}\")\n",
    "        print(\"Train a model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 experiment results from CSV\n",
      " Best model copied from Experiment 2\n",
      "  Source: benitha_models/benitha_exp_2_CnnPolicy/best_model.zip\n",
      "  Destination: benitha_dqn_model.zip\n",
      "  Best Reward: 35.40\n",
      "\n",
      " benitha_dqn_model.zip is ready for play.py\n",
      "  File size: 178.01 MB\n"
     ]
    }
   ],
   "source": [
    "# Select and save the best model as benitha_dqn_model.zip for use in play.py\n",
    "# This is REQUIRED for the assignment\n",
    "\n",
    "# Load results from CSV if available (works even after notebook restart)\n",
    "if os.path.exists('benitha_experiment_results.csv'):\n",
    "    try:\n",
    "        results_df = pd.read_csv('benitha_experiment_results.csv')\n",
    "        all_results = results_df.to_dict('records')\n",
    "        print(f\"Loaded {len(all_results)} experiment results from CSV\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results: {e}\")\n",
    "        all_results = []\n",
    "elif 'all_results' not in globals():\n",
    "    all_results = []\n",
    "\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    successful = results_df[results_df['status'] == 'success'].copy()\n",
    "    \n",
    "    if len(successful) > 0:\n",
    "        # Find the best performing experiment based on best_mean_reward\n",
    "        if 'best_mean_reward' in successful.columns:\n",
    "            valid_rewards = successful['best_mean_reward'].dropna()\n",
    "            if len(valid_rewards) > 0:\n",
    "                best_exp_idx = valid_rewards.idxmax()\n",
    "                best_exp = successful.loc[best_exp_idx]\n",
    "                best_exp_num = int(best_exp['experiment'])\n",
    "                \n",
    "                # Path to best model\n",
    "                best_model_path = f\"benitha_models/benitha_exp_{best_exp_num}_CnnPolicy/best_model.zip\"\n",
    "                final_model_path = f\"benitha_models/benitha_exp_{best_exp_num}_CnnPolicy/benitha_dqn_model.zip\"\n",
    "                \n",
    "                # Try best_model first, then final model\n",
    "                if os.path.exists(best_model_path):\n",
    "                    import shutil\n",
    "                    shutil.copy(best_model_path, \"benitha_dqn_model.zip\")\n",
    "                    print(f\" Best model copied from Experiment {best_exp_num}\")\n",
    "                    print(f\"  Source: {best_model_path}\")\n",
    "                    print(f\"  Destination: benitha_dqn_model.zip\")\n",
    "                    print(f\"  Best Reward: {best_exp['best_mean_reward']:.2f}\")\n",
    "                elif os.path.exists(final_model_path):\n",
    "                    import shutil\n",
    "                    shutil.copy(final_model_path, \"benitha_dqn_model.zip\")\n",
    "                    print(f\" Final model copied from Experiment {best_exp_num}\")\n",
    "                    print(f\"  Source: {final_model_path}\")\n",
    "                    print(f\"  Destination: benitha_dqn_model.zip\")\n",
    "                    print(f\"  Best Reward: {best_exp['best_mean_reward']:.2f}\")\n",
    "                else:\n",
    "                    print(f\" Model files not found for Experiment {best_exp_num}\")\n",
    "                    print(f\"  Expected: {best_model_path} or {final_model_path}\")\n",
    "                    print(f\"  Please check if training completed successfully.\")\n",
    "            else:\n",
    "                print(\"No reward data available. Using first successful experiment.\")\n",
    "                first_exp = successful.iloc[0]\n",
    "                first_exp_num = int(first_exp['experiment'])\n",
    "                model_path = f\"benitha_models/benitha_exp_{first_exp_num}_CnnPolicy/benitha_dqn_model.zip\"\n",
    "                if os.path.exists(model_path):\n",
    "                    import shutil\n",
    "                    shutil.copy(model_path, \"benitha_dqn_model.zip\")\n",
    "                    print(f\" Model copied from Experiment {first_exp_num}\")\n",
    "                else:\n",
    "                    print(f\" Model not found at {model_path}\")\n",
    "        else:\n",
    "            print(\"No reward data available. Using first successful experiment.\")\n",
    "            first_exp = successful.iloc[0]\n",
    "            first_exp_num = int(first_exp['experiment'])\n",
    "            model_path = f\"benitha_models/benitha_exp_{first_exp_num}_CnnPolicy/benitha_dqn_model.zip\"\n",
    "            if os.path.exists(model_path):\n",
    "                import shutil\n",
    "                shutil.copy(model_path, \"benitha_dqn_model.zip\")\n",
    "                print(f\" Model copied from Experiment {first_exp_num}\")\n",
    "            else:\n",
    "                print(f\" Model not found at {model_path}\")\n",
    "        \n",
    "        # Verify the file exists\n",
    "        if os.path.exists(\"benitha_dqn_model.zip\"):\n",
    "            file_size = os.path.getsize(\"benitha_dqn_model.zip\") / (1024 * 1024)  # MB\n",
    "            print(f\"\\n benitha_dqn_model.zip is ready for play.py\")\n",
    "            print(f\"  File size: {file_size:.2f} MB\")\n",
    "        else:\n",
    "            print(\"\\n benitha_dqn_model.zip was not created. Please check model paths.\")\n",
    "    else:\n",
    "        print(\"No successful experiments found. Run experiments first.\")\n",
    "else:\n",
    "    print(\"No results available. Run experiments first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
