{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Training Experiments for Bowling Atari Environment\n",
    "\n",
    "**Group Member Name:** Afsa Umutoniwase\n",
    "\n",
    "This notebook loads the Bowling environment and runs **10 training experiments** with different hyperparameter configurations from the CSV file. Each experiment tests different combinations of:\n",
    "- Learning Rate (lr)\n",
    "- Gamma (Î³) - Discount factor\n",
    "- Batch Size\n",
    "- Epsilon parameters (start, end, decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import ale_py to register ALE namespace\n",
    "import ale_py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Bowling environment...\n",
      "Environment loaded: <Monitor<OrderEnforcing<PassiveEnvChecker<AtariEnv<ALE/Bowling-v5>>>>>\n",
      "Action space: Discrete(6)\n",
      "Observation space: Box(0, 255, (210, 160, 3), uint8)\n",
      "\n",
      "Observation shape: (210, 160, 3)\n",
      "Environment is ready!\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_id=\"ALE/Bowling-v5\", render_mode=None):\n",
    "    \"\"\"Create and wrap the environment\"\"\"\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "# Test environment loading\n",
    "print(\"Loading Bowling environment...\")\n",
    "test_env = make_env()\n",
    "print(f\"Environment loaded: {test_env}\")\n",
    "print(f\"Action space: {test_env.action_space}\")\n",
    "print(f\"Observation space: {test_env.observation_space}\")\n",
    "\n",
    "# Test a random action\n",
    "obs, info = test_env.reset()\n",
    "print(f\"\\nObservation shape: {obs.shape}\")\n",
    "print(\"Environment is ready!\")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Hyperparameters from CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded hyperparameter configurations:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>#</th>\n",
       "      <th>lr (Learning Rate)</th>\n",
       "      <th>gamma (Discount Factor)</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epsilon_start</th>\n",
       "      <th>epsilon_end</th>\n",
       "      <th>epsilon_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.95</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.99</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.97</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.90</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.92</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.98</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.96</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.93</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.99</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   #  lr (Learning Rate)  gamma (Discount Factor)  batch_size  \\\n",
       "0         NaN   1              0.0003                     0.95          32   \n",
       "1         NaN   2              0.0010                     0.99         128   \n",
       "2         NaN   3              0.0005                     0.97          64   \n",
       "3         NaN   4              0.0025                     0.90          32   \n",
       "4         NaN   5              0.0001                     0.99          64   \n",
       "5         NaN   6              0.0015                     0.92         128   \n",
       "6         NaN   7              0.0007                     0.98          32   \n",
       "7         NaN   8              0.0020                     0.96          64   \n",
       "8         NaN   9              0.0004                     0.93         128   \n",
       "9         NaN  10              0.0035                     0.99          32   \n",
       "\n",
       "   epsilon_start  epsilon_end  epsilon_decay  \n",
       "0              1         0.01        0.00005  \n",
       "1              1         0.05        0.00002  \n",
       "2              1         0.10        0.00010  \n",
       "3              1         0.02        0.00001  \n",
       "4              1         0.05        0.00010  \n",
       "5              1         0.10        0.00005  \n",
       "6              1         0.01        0.00002  \n",
       "7              1         0.05        0.00010  \n",
       "8              1         0.10        0.00050  \n",
       "9              1         0.02        0.00003  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepared 10 experiments for training\n",
      "\n",
      "Experiment configurations:\n",
      "  Exp 1: lr=0.0003, gamma=0.95, batch=32, eps=1.0->0.01, decay=5e-05\n",
      "  Exp 2: lr=0.001, gamma=0.99, batch=128, eps=1.0->0.05, decay=2e-05\n",
      "  Exp 3: lr=0.0005, gamma=0.97, batch=64, eps=1.0->0.1, decay=0.0001\n",
      "  Exp 4: lr=0.0025, gamma=0.9, batch=32, eps=1.0->0.02, decay=1e-05\n",
      "  Exp 5: lr=0.0001, gamma=0.99, batch=64, eps=1.0->0.05, decay=0.0001\n",
      "  Exp 6: lr=0.0015, gamma=0.92, batch=128, eps=1.0->0.1, decay=5e-05\n",
      "  Exp 7: lr=0.0007, gamma=0.98, batch=32, eps=1.0->0.01, decay=2e-05\n",
      "  Exp 8: lr=0.002, gamma=0.96, batch=64, eps=1.0->0.05, decay=0.0001\n",
      "  Exp 9: lr=0.0004, gamma=0.93, batch=128, eps=1.0->0.1, decay=0.0005\n",
      "  Exp 10: lr=0.0035, gamma=0.99, batch=32, eps=1.0->0.02, decay=3e-05\n"
     ]
    }
   ],
   "source": [
    "# Load hyperparameters from CSV\n",
    "csv_file = \"Afsa parameters for experiments.csv\"\n",
    "\n",
    "# Read CSV, skipping the first empty row\n",
    "df = pd.read_csv(csv_file, skiprows=1, skipinitialspace=True)\n",
    "\n",
    "# Clean column names (remove spaces and special characters)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Remove completely empty rows\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "# Display the loaded configurations\n",
    "print(\"Loaded hyperparameter configurations:\")\n",
    "print(\"=\"*80)\n",
    "display(df)\n",
    "\n",
    "# Prepare experiments list\n",
    "experiments = []\n",
    "for idx, row in df.iterrows():\n",
    "    # Check if this row has valid data\n",
    "    try:\n",
    "        # Get experiment number - handle both '#' and numeric index\n",
    "        exp_num = None\n",
    "        if '#' in row.index:\n",
    "            exp_num = row['#']\n",
    "        elif 0 in row.index:\n",
    "            exp_num = row.iloc[0]\n",
    "        \n",
    "        if pd.isna(exp_num) or exp_num == '':\n",
    "            continue\n",
    "        \n",
    "        # Try to convert to int to verify it's a number (not a header)\n",
    "        exp_num = int(float(exp_num))\n",
    "        \n",
    "        # Get learning rate\n",
    "        lr = None\n",
    "        if 'lr (Learning Rate)' in row.index:\n",
    "            lr = row['lr (Learning Rate)']\n",
    "        elif 'lr' in row.index:\n",
    "            lr = row['lr']\n",
    "        \n",
    "        if pd.isna(lr) or lr == '':\n",
    "            continue\n",
    "        \n",
    "        # Build experiment dictionary\n",
    "        exp = {\n",
    "            'num': exp_num,\n",
    "            'lr': float(row['lr (Learning Rate)']),\n",
    "            'gamma': float(row['gamma (Discount Factor)']),\n",
    "            'batch_size': int(row['batch_size']),\n",
    "            'epsilon_start': float(row['epsilon_start']),\n",
    "            'epsilon_end': float(row['epsilon_end']),\n",
    "            'epsilon_decay': float(row['epsilon_decay'])\n",
    "        }\n",
    "        experiments.append(exp)\n",
    "    except (ValueError, TypeError, KeyError) as e:\n",
    "        # Skip rows that can't be parsed (likely header or invalid rows)\n",
    "        print(f\"  Skipping row {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nPrepared {len(experiments)} experiments for training\")\n",
    "if len(experiments) > 0:\n",
    "    print(\"\\nExperiment configurations:\")\n",
    "    for exp in experiments:\n",
    "        print(f\"  Exp {exp['num']}: lr={exp['lr']}, gamma={exp['gamma']}, batch={exp['batch_size']}, \"\n",
    "              f\"eps={exp['epsilon_start']}->{exp['epsilon_end']}, decay={exp['epsilon_decay']}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No experiments loaded! Check CSV file format.\")\n",
    "    print(\"Expected columns: #, lr (Learning Rate), gamma (Discount Factor), batch_size, epsilon_start, epsilon_end, epsilon_decay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_epsilon_decay(epsilon_decay):\n",
    "    \"\"\"Convert epsilon_decay rate to exploration_fraction\"\"\"\n",
    "    if epsilon_decay < 0.0001:\n",
    "        return 0.8  # Very slow decay -> decay over most of training\n",
    "    elif epsilon_decay < 0.0005:\n",
    "        return 0.5  # Slow decay -> decay over middle portion\n",
    "    else:\n",
    "        return 0.2  # Fast decay -> decay over early portion\n",
    "\n",
    "def train_experiment(exp_config, total_timesteps=50000, policy_type=\"CnnPolicy\"):\n",
    "    \"\"\"\n",
    "    Train a DQN agent with specified hyperparameters\n",
    "    \n",
    "    Args:\n",
    "        exp_config: Dictionary with hyperparameters\n",
    "        total_timesteps: Total training timesteps\n",
    "        policy_type: \"CnnPolicy\" or \"MlpPolicy\"\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        log_dir: Path to logs\n",
    "        model_dir: Path to saved model\n",
    "        training_info: Dictionary with training statistics\n",
    "    \"\"\"\n",
    "    exp_num = exp_config['num']\n",
    "    \n",
    "    print(f\"EXPERIMENT {exp_num}\")\n",
    "    print(f\"Policy: {policy_type}\")\n",
    "    print(f\"Learning Rate: {exp_config['lr']}\")\n",
    "    print(f\"Gamma: {exp_config['gamma']}\")\n",
    "    print(f\"Batch Size: {exp_config['batch_size']}\")\n",
    "    print(f\"Epsilon: {exp_config['epsilon_start']} -> {exp_config['epsilon_end']}\")\n",
    "    \n",
    "    # Convert epsilon_decay to exploration_fraction\n",
    "    exploration_fraction = convert_epsilon_decay(exp_config['epsilon_decay'])\n",
    "    print(f\"Epsilon Decay Rate: {exp_config['epsilon_decay']} -> Exploration Fraction: {exploration_fraction}\")\n",
    "    print(f\"Total Timesteps: {total_timesteps}\")\n",
    "    \n",
    "    # Create directories\n",
    "    experiment_name = f\"exp_{exp_num}\"\n",
    "    log_dir = f\"logs/{experiment_name}_{policy_type}\"\n",
    "    model_dir = f\"models/{experiment_name}_{policy_type}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Create environments\n",
    "    env = DummyVecEnv([lambda: make_env()])\n",
    "    eval_env = DummyVecEnv([lambda: make_env()])\n",
    "    \n",
    "    # Initialize DQN agent\n",
    "    # Use smaller buffer_size for Atari to avoid memory issues (default is 1M which is too large)\n",
    "    model = DQN(\n",
    "        policy_type,\n",
    "        env,\n",
    "        learning_rate=exp_config['lr'],\n",
    "        gamma=exp_config['gamma'],\n",
    "        batch_size=exp_config['batch_size'],\n",
    "        buffer_size=100000,  # Reduced from default 1M to save memory (9.4GB instead of 94GB)\n",
    "        exploration_initial_eps=exp_config['epsilon_start'],\n",
    "        exploration_final_eps=exp_config['epsilon_end'],\n",
    "        exploration_fraction=exploration_fraction,\n",
    "        verbose=1,\n",
    "        tensorboard_log=log_dir,\n",
    "        device=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Setup callbacks\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=model_dir,\n",
    "        log_path=log_dir,\n",
    "        eval_freq=5000,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=10000,\n",
    "        save_path=model_dir,\n",
    "        name_prefix=\"dqn_checkpoint\"\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=[eval_callback, checkpoint_callback],\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = f\"{model_dir}/dqn_model.zip\"\n",
    "    model.save(final_model_path)\n",
    "    print(f\"\\nModel saved to: {final_model_path}\")\n",
    "    \n",
    "    # Extract training statistics from monitor\n",
    "    training_info = {\n",
    "        'experiment': exp_num,\n",
    "        'policy': policy_type,\n",
    "        'final_timesteps': total_timesteps\n",
    "    }\n",
    "    \n",
    "    # Try to get evaluation results\n",
    "    if hasattr(eval_callback, 'best_mean_reward'):\n",
    "        training_info['best_mean_reward'] = eval_callback.best_mean_reward\n",
    "        training_info['n_eval_episodes'] = eval_callback.n_eval_episodes\n",
    "    \n",
    "    return model, log_dir, model_dir, training_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Individual Experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration:\n",
      "  Total timesteps per experiment: 30000\n",
      "  Policy type: CnnPolicy\n",
      "  Total experiments to run: 10\n",
      "  Estimated total time: ~300 minutes\n",
      "\n",
      "Note: Completed experiments will be skipped automatically.\n"
     ]
    }
   ],
   "source": [
    "# Configuration for all experiments\n",
    "TOTAL_TIMESTEPS = 30000  # Adjust based on available time\n",
    "POLICY_TYPE = \"CnnPolicy\"  # Use \"MlpPolicy\" for comparison\n",
    "\n",
    "# Load existing results if available (prevents re-running completed experiments)\n",
    "if os.path.exists('experiment_results.csv'):\n",
    "    try:\n",
    "        existing_df = pd.read_csv('experiment_results.csv')\n",
    "        all_results = existing_df.to_dict('records')\n",
    "        print(f\"Loaded {len(all_results)} existing experiment results from CSV\")\n",
    "        successful = [r for r in all_results if r.get('status') == 'success']\n",
    "        print(f\"  - {len(successful)} successful experiments\")\n",
    "        print(f\"  - {len(all_results) - len(successful)} failed/incomplete experiments\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load existing results: {e}\")\n",
    "        all_results = []\n",
    "        all_training_info = []\n",
    "else:\n",
    "    all_results = []\n",
    "    all_training_info = []\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Total timesteps per experiment: {TOTAL_TIMESTEPS}\")\n",
    "print(f\"  Policy type: {POLICY_TYPE}\")\n",
    "print(f\"  Total experiments to run: 10\")\n",
    "print(f\"  Estimated total time: ~{10 * TOTAL_TIMESTEPS / 1000:.0f} minutes\")\n",
    "print(f\"\\nNote: Completed experiments will be skipped automatically.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 1\n",
      "Policy: CnnPolicy\n",
      "Learning Rate: 0.0003\n",
      "Gamma: 0.95\n",
      "Batch Size: 32\n",
      "Epsilon: 1.0 -> 0.01\n",
      "Epsilon Decay Rate: 5e-05 -> Exploration Fraction: 0.8\n",
      "Total Timesteps: 30000\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Starting training...\n",
      "Logging to logs/exp_1_CnnPolicy\\DQN_4\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: lr=0.0003, gamma=0.95, batch=32\n",
    "# Epsilon: 1.0 -> 0.01, decay=5e-05\n",
    "\n",
    "# Check if already completed\n",
    "exp_1_completed = any(r.get('experiment') == 1 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_1_completed:\n",
    "    print(\"Experiment 1 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 1,\n",
    "            'lr': 0.0003,\n",
    "            'gamma': 0.95,\n",
    "            'batch_size': 32,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.01,\n",
    "            'epsilon_decay': 5e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 1,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('experiment_results.csv', index=False)\n",
    "        all_training_info.append(training_info)\n",
    "        \n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Experiment 1 completed in {duration:.2f} minutes\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        print(f\"  Model saved to: {model_dir}/dqn_model.zip\")\n",
    "        print(f\"  Results saved to: experiment_results.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 1: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 1,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: lr=0.001, gamma=0.99, batch=128\n",
    "# Epsilon: 1.0 -> 0.05, decay=2e-05\n",
    "\n",
    "# Check if already completed\n",
    "exp_2_completed = any(r.get('experiment') == 2 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_2_completed:\n",
    "    print(\"Experiment 2 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 2,\n",
    "            'lr': 0.001,\n",
    "            'gamma': 0.99,\n",
    "            'batch_size': 128,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.05,\n",
    "            'epsilon_decay': 2e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 2,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('experiment_results.csv', index=False)\n",
    "        all_training_info.append(training_info)\n",
    "        \n",
    "        print(f\"Experiment 2 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/dqn_model.zip\")\n",
    "        print(f\"  Results saved to: experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 2: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 2,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: lr=0.0005, gamma=0.97, batch=64\n",
    "# Epsilon: 1.0 -> 0.1, decay=0.0001\n",
    "\n",
    "# Check if already completed\n",
    "exp_3_completed = any(r.get('experiment') == 3 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_3_completed:\n",
    "    print(\"Experiment 3 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 3,\n",
    "            'lr': 0.0005,\n",
    "            'gamma': 0.97,\n",
    "            'batch_size': 64,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.1,\n",
    "            'epsilon_decay': 0.0001\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 3,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('experiment_results.csv', index=False)\n",
    "        all_training_info.append(training_info)\n",
    "        \n",
    "        print(f\"Experiment 3 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/dqn_model.zip\")\n",
    "        print(f\"  Results saved to: experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 3: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 3,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: lr=0.0025, gamma=0.9, batch=32\n",
    "# Epsilon: 1.0 -> 0.02, decay=1e-05\n",
    "\n",
    "# Check if already completed\n",
    "exp_4_completed = any(r.get('experiment') == 4 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_4_completed:\n",
    "    print(\"Experiment 4 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 4,\n",
    "            'lr': 0.0025,\n",
    "            'gamma': 0.9,\n",
    "            'batch_size': 32,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.02,\n",
    "            'epsilon_decay': 1e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 4,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('experiment_results.csv', index=False)\n",
    "        all_training_info.append(training_info)\n",
    "        \n",
    "        print(f\"Experiment 4 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/dqn_model.zip\")\n",
    "        print(f\"  Results saved to: experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 4: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 4,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: lr=0.0001, gamma=0.99, batch=64\n",
    "# Epsilon: 1.0 -> 0.05, decay=0.0001\n",
    "\n",
    "# Check if already completed\n",
    "exp_5_completed = any(r.get('experiment') == 5 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_5_completed:\n",
    "    print(\"Experiment 5 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 5,\n",
    "            'lr': 0.0001,\n",
    "            'gamma': 0.99,\n",
    "            'batch_size': 64,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.05,\n",
    "            'epsilon_decay': 0.0001\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 5,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('experiment_results.csv', index=False)\n",
    "        all_training_info.append(training_info)\n",
    "        \n",
    "        print(f\"Experiment 5 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/dqn_model.zip\")\n",
    "        print(f\"  Results saved to: experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 5: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 5,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6: lr=0.0015, gamma=0.92, batch=128\n",
    "# Epsilon: 1.0 -> 0.1, decay=5e-05\n",
    "\n",
    "# Check if already completed\n",
    "exp_6_completed = any(r.get('experiment') == 6 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_6_completed:\n",
    "    print(\"Experiment 6 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 6,\n",
    "            'lr': 0.0015,\n",
    "            'gamma': 0.92,\n",
    "            'batch_size': 128,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.1,\n",
    "            'epsilon_decay': 5e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 6,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('experiment_results.csv', index=False)\n",
    "        all_training_info.append(training_info)\n",
    "        \n",
    "        print(f\"Experiment 6 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/dqn_model.zip\")\n",
    "        print(f\"  Results saved to: experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 6: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 6,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 7: lr=0.0007, gamma=0.98, batch=32\n",
    "# Epsilon: 1.0 -> 0.01, decay=2e-05\n",
    "\n",
    "# Check if already completed\n",
    "exp_7_completed = any(r.get('experiment') == 7 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_7_completed:\n",
    "    print(\"Experiment 7 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 7,\n",
    "            'lr': 0.0007,\n",
    "            'gamma': 0.98,\n",
    "            'batch_size': 32,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.01,\n",
    "            'epsilon_decay': 2e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 7,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('experiment_results.csv', index=False)\n",
    "        all_training_info.append(training_info)\n",
    "        \n",
    "        print(f\"Experiment 7 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/dqn_model.zip\")\n",
    "        print(f\"  Results saved to: experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 7: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 7,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 8: lr=0.002, gamma=0.96, batch=64\n",
    "# Epsilon: 1.0 -> 0.05, decay=0.0001\n",
    "\n",
    "# Check if already completed\n",
    "exp_8_completed = any(r.get('experiment') == 8 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_8_completed:\n",
    "    print(\"Experiment 8 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 8,\n",
    "            'lr': 0.002,\n",
    "            'gamma': 0.96,\n",
    "            'batch_size': 64,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.05,\n",
    "            'epsilon_decay': 0.0001\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 8,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('experiment_results.csv', index=False)\n",
    "        all_training_info.append(training_info)\n",
    "        \n",
    "        print(f\"Experiment 8 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/dqn_model.zip\")\n",
    "        print(f\"  Results saved to: experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 8: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 8,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 9: lr=0.0004, gamma=0.93, batch=128\n",
    "# Epsilon: 1.0 -> 0.1, decay=0.0005\n",
    "\n",
    "# Check if already completed\n",
    "exp_9_completed = any(r.get('experiment') == 9 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_9_completed:\n",
    "    print(\"Experiment 9 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 9,\n",
    "            'lr': 0.0004,\n",
    "            'gamma': 0.93,\n",
    "            'batch_size': 128,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.1,\n",
    "            'epsilon_decay': 0.0005\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 9,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('experiment_results.csv', index=False)\n",
    "        all_training_info.append(training_info)\n",
    "        \n",
    "        print(f\"Experiment 9 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/dqn_model.zip\")\n",
    "        print(f\"  Results saved to: experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 9: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 9,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 10: lr=0.0035, gamma=0.99, batch=32\n",
    "# Epsilon: 1.0 -> 0.02, decay=3e-05\n",
    "\n",
    "# Check if already completed\n",
    "exp_10_completed = any(r.get('experiment') == 10 and r.get('status') == 'success' for r in all_results)\n",
    "\n",
    "if exp_10_completed:\n",
    "    print(\"Experiment 10 already completed. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        exp_config = {\n",
    "            'num': 10,\n",
    "            'lr': 0.0035,\n",
    "            'gamma': 0.99,\n",
    "            'batch_size': 32,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_end': 0.02,\n",
    "            'epsilon_decay': 3e-05\n",
    "        }\n",
    "        \n",
    "        model, log_dir, model_dir, training_info = train_experiment(\n",
    "            exp_config,\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            policy_type=POLICY_TYPE\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60  # in minutes\n",
    "        \n",
    "        result = {\n",
    "            'experiment': 10,\n",
    "            'status': 'success',\n",
    "            'duration_minutes': duration,\n",
    "            'log_dir': log_dir,\n",
    "            'model_dir': model_dir,\n",
    "            'best_mean_reward': training_info.get('best_mean_reward', None),\n",
    "            **exp_config\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "        # Save results to CSV after each experiment\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv('experiment_results.csv', index=False)\n",
    "        all_training_info.append(training_info)\n",
    "        \n",
    "        print(f\"Experiment 10 completed in {duration:.2f} minutes\")\n",
    "        print(f\"  Model saved to: {model_dir}/dqn_model.zip\")\n",
    "        print(f\"  Results saved to: experiment_results.csv\")\n",
    "        if training_info.get('best_mean_reward') is not None:\n",
    "            print(f\"  Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in experiment 10: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result = {\n",
    "            'experiment': 10,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            **exp_config\n",
    "        }\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Experiments Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all completed experiments\n",
    "# Load results from CSV if available\n",
    "if os.path.exists('experiment_results.csv'):\n",
    "    try:\n",
    "        results_df = pd.read_csv('experiment_results.csv')\n",
    "        all_results = results_df.to_dict('records')\n",
    "    except:\n",
    "        all_results = []\n",
    "elif 'all_results' in globals():\n",
    "    # Use in-memory results\n",
    "    pass\n",
    "else:\n",
    "    all_results = []\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    successful = [r for r in all_results if r.get('status') == 'success']\n",
    "    failed = [r for r in all_results if r.get('status') == 'failed']\n",
    "    \n",
    "    print(f\"EXPERIMENTS SUMMARY\")\n",
    "    print(f\"\\nCompleted: {len(successful)}/10\")\n",
    "    print(f\"Failed: {len(failed)}/10\")\n",
    "    \n",
    "    if len(successful) > 0:\n",
    "        print(f\"\\nSuccessful experiments: {sorted([r['experiment'] for r in successful])}\")\n",
    "        # Show rewards if available\n",
    "        rewards = [r.get('best_mean_reward') for r in successful if pd.notna(r.get('best_mean_reward'))]\n",
    "        if rewards:\n",
    "            print(f\"\\nReward Statistics:\")\n",
    "            print(f\"  Best: {max(rewards):.2f}\")\n",
    "            print(f\"  Average: {sum(rewards)/len(rewards):.2f}\")\n",
    "    if len(failed) > 0:\n",
    "        print(f\"\\nFailed experiments: {sorted([r['experiment'] for r in failed])}\")\n",
    "    \n",
    "    # Save results to CSV for persistence\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv('experiment_results.csv', index=False)\n",
    "    print(f\"\\nResults saved to experiment_results.csv\")\n",
    "else:\n",
    "    print(\"No experiments completed yet. Run the experiment cells above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare MLPPolicy vs CNNPolicy (Required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare policies using the first experiment configuration\n",
    "# This is REQUIRED for the assignment - compare MLPPolicy vs CNNPolicy\n",
    "if len(experiments) > 0:\n",
    "    comparison_exp = experiments[0].copy()\n",
    "    \n",
    "    print(\"COMPARING MLPPolicy vs CNNPolicy\")\n",
    "    print(f\"Using experiment {comparison_exp['num']} configuration\")\n",
    "    print(\"This comparison is required for the assignment.\")\n",
    "    \n",
    "    comparison_results = {}\n",
    "    \n",
    "    for policy in [\"MlpPolicy\", \"CnnPolicy\"]:\n",
    "        print(f\"Training with {policy}\")\n",
    "        \n",
    "        try:\n",
    "            model, log_dir, model_dir, training_info = train_experiment(\n",
    "                comparison_exp,\n",
    "                total_timesteps=20000,  # Reduced for comparison\n",
    "                policy_type=policy\n",
    "            )\n",
    "            \n",
    "            comparison_results[policy] = {\n",
    "                'status': 'success',\n",
    "                'log_dir': log_dir,\n",
    "                'model_dir': model_dir,\n",
    "                'best_mean_reward': training_info.get('best_mean_reward', None)\n",
    "            }\n",
    "            \n",
    "            if training_info.get('best_mean_reward') is not None:\n",
    "                print(f\"\\n{policy} Best Mean Reward: {training_info['best_mean_reward']:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {policy}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            comparison_results[policy] = {\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    print(\"POLICY COMPARISON COMPLETE\")\n",
    "    print(\"\\nResults Summary:\")\n",
    "    for policy, result in comparison_results.items():\n",
    "        if result['status'] == 'success':\n",
    "            reward_info = f\"Best Reward: {result.get('best_mean_reward', 'N/A')}\" if result.get('best_mean_reward') else \"Reward: N/A\"\n",
    "            print(f\"  {policy}: {reward_info}\")\n",
    "            print(f\"    Logs: {result['log_dir']}\")\n",
    "    print(\"\\nView TensorBoard logs to compare performance:\")\n",
    "    print(\"  tensorboard --logdir logs/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "    \n",
    "    # Display key columns\n",
    "    display_cols = ['experiment', 'status', 'lr', 'gamma', 'batch_size', \n",
    "                    'epsilon_start', 'epsilon_end', 'epsilon_decay', \n",
    "                    'best_mean_reward', 'duration_minutes']\n",
    "    available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "    display(results_df[available_cols])\n",
    "    \n",
    "    # Summary statistics\n",
    "    successful = results_df[results_df['status'] == 'success']\n",
    "    if len(successful) > 0:\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(f\"Successful experiments: {len(successful)}\")\n",
    "        print(f\"Average training time: {successful['duration_minutes'].mean():.2f} minutes\")\n",
    "        print(f\"Total training time: {successful['duration_minutes'].sum():.2f} minutes\")\n",
    "        \n",
    "        if 'best_mean_reward' in successful.columns:\n",
    "            valid_rewards = successful['best_mean_reward'].dropna()\n",
    "            if len(valid_rewards) > 0:\n",
    "                print(f\"\\nReward Statistics:\")\n",
    "                print(f\"  Best reward: {valid_rewards.max():.2f}\")\n",
    "                print(f\"  Worst reward: {valid_rewards.min():.2f}\")\n",
    "                print(f\"  Average reward: {valid_rewards.mean():.2f}\")\n",
    "                print(f\"  Median reward: {valid_rewards.median():.2f}\")\n",
    "    \n",
    "    failed = results_df[results_df['status'] == 'failed']\n",
    "    if len(failed) > 0:\n",
    "        print(f\"\\nFailed experiments: {len(failed)}\")\n",
    "        for idx, row in failed.iterrows():\n",
    "            print(f\"  Experiment {row['experiment']}: {row.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv('experiment_results.csv', index=False)\n",
    "    print(\"\\n Results saved to experiment_results.csv\")\n",
    "else:\n",
    "    print(\"No results to display. Run experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze and generate observed behaviors\n",
    "def analyze_experiment_behavior(row, all_successful):\n",
    "    \"\"\"\n",
    "    Automatically analyze experiment results and generate behavior description\n",
    "    Args:\n",
    "        row: Single experiment row from results DataFrame\n",
    "        all_successful: All successful experiments for comparison\n",
    "    Returns:\n",
    "        behavior_description: String describing observed behavior\n",
    "    \"\"\"\n",
    "    behaviors = []\n",
    "    \n",
    "    # Get reward information\n",
    "    best_reward = row.get('best_mean_reward')\n",
    "    if pd.notna(best_reward):\n",
    "        # Compare with other experiments\n",
    "        all_rewards = all_successful['best_mean_reward'].dropna()\n",
    "        if len(all_rewards) > 0:\n",
    "            reward_percentile = (all_rewards <= best_reward).sum() / len(all_rewards) * 100\n",
    "            if reward_percentile >= 80:\n",
    "                behaviors.append(f\"Excellent performance (top {100-int(reward_percentile)}%)\")\n",
    "            elif reward_percentile >= 60:\n",
    "                behaviors.append(f\"Good performance (above average)\")\n",
    "            elif reward_percentile >= 40:\n",
    "                behaviors.append(f\"Moderate performance (average)\")\n",
    "            else:\n",
    "                behaviors.append(f\"Below average performance\")\n",
    "            behaviors.append(f\"Best reward: {best_reward:.2f}\")\n",
    "        else:\n",
    "            behaviors.append(f\"Best reward: {best_reward:.2f}\")\n",
    "    \n",
    "    # Analyze learning rate\n",
    "    lr = row['lr']\n",
    "    if lr > 0.002:\n",
    "        behaviors.append(\"High LR - may cause instability\")\n",
    "    elif lr < 0.0002:\n",
    "        behaviors.append(\"Low LR - slower but stable learning\")\n",
    "    else:\n",
    "        behaviors.append(\"Moderate LR - balanced learning\")\n",
    "    \n",
    "    # Analyze gamma (discount factor)\n",
    "    gamma = row['gamma']\n",
    "    if gamma >= 0.98:\n",
    "        behaviors.append(\"High gamma - long-term focus\")\n",
    "    elif gamma <= 0.92:\n",
    "        behaviors.append(\"Low gamma - short-term focus\")\n",
    "    else:\n",
    "        behaviors.append(\"Moderate gamma - balanced\")\n",
    "    \n",
    "    # Analyze batch size\n",
    "    batch = row['batch_size']\n",
    "    if batch >= 128:\n",
    "        behaviors.append(\"Large batch - stable gradients\")\n",
    "    elif batch <= 32:\n",
    "        behaviors.append(\"Small batch - faster updates\")\n",
    "    else:\n",
    "        behaviors.append(\"Medium batch - balanced\")\n",
    "    \n",
    "    # Analyze epsilon decay\n",
    "    eps_decay = row['epsilon_decay']\n",
    "    if eps_decay < 0.0001:\n",
    "        behaviors.append(\"Slow exploration decay - more exploration\")\n",
    "    elif eps_decay > 0.0003:\n",
    "        behaviors.append(\"Fast exploration decay - quick exploitation\")\n",
    "    else:\n",
    "        behaviors.append(\"Moderate exploration decay\")\n",
    "    \n",
    "    # Analyze training duration (if available)\n",
    "    if 'duration_minutes' in row and pd.notna(row['duration_minutes']):\n",
    "        duration = row['duration_minutes']\n",
    "        avg_duration = all_successful['duration_minutes'].mean()\n",
    "        if duration < avg_duration * 0.9:\n",
    "            behaviors.append(\"Faster training\")\n",
    "        elif duration > avg_duration * 1.1:\n",
    "            behaviors.append(\"Slower training\")\n",
    "    \n",
    "    # Combine all behaviors\n",
    "    if behaviors:\n",
    "        return \". \".join(behaviors) + \".\"\n",
    "    else:\n",
    "        return \"Training completed. Review TensorBoard for detailed metrics.\"\n",
    "\n",
    "# Generate hyperparameter table for documentation\n",
    "def create_hyperparameter_table(results_df, auto_fill=True):\n",
    "    \"\"\"Create formatted table for README documentation with behavior analysis\"\"\"\n",
    "    if results_df is None or len(results_df) == 0:\n",
    "        print(\"No results available. Run experiments first.\")\n",
    "        return\n",
    "    \n",
    "    successful = results_df[results_df['status'] == 'success'].copy()\n",
    "    \n",
    "    if len(successful) == 0:\n",
    "        print(\"No successful experiments to document.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*120)\n",
    "    print(\"HYPERPARAMETER TUNING TABLE - COPY TO README.md\")\n",
    "    if auto_fill:\n",
    "        print(\"(Observed Behaviors generated from training results)\")\n",
    "    print(\"=\"*120)\n",
    "    print(\"\\n| Experiment # | Learning Rate | Gamma | Batch Size | Epsilon Start | Epsilon End | Epsilon Decay | Observed Behavior |\")\n",
    "    print(\"|--------------|---------------|-------|------------|---------------|-------------|---------------|-------------------|\")\n",
    "    \n",
    "    for idx, row in successful.iterrows():\n",
    "        exp_num = int(row['experiment'])\n",
    "        lr = row['lr']\n",
    "        gamma = row['gamma']\n",
    "        batch = int(row['batch_size'])\n",
    "        eps_start = row['epsilon_start']\n",
    "        eps_end = row['epsilon_end']\n",
    "        eps_decay = row['epsilon_decay']\n",
    "        \n",
    "        # Generate automatic behavior description\n",
    "        if auto_fill:\n",
    "            behavior = analyze_experiment_behavior(row, successful)\n",
    "        else:\n",
    "            # Get reward info if available\n",
    "            reward_info = \"\"\n",
    "            if pd.notna(row.get('best_mean_reward')):\n",
    "                reward_info = f\"Best reward: {row['best_mean_reward']:.2f}. \"\n",
    "            behavior = f\"{reward_info}[DESCRIBE: Convergence speed, stability, final performance, reward trends, episode lengths]\"\n",
    "        \n",
    "        # Truncate behavior if too long for table\n",
    "        if len(behavior) > 100:\n",
    "            behavior = behavior[:97] + \"...\"\n",
    "        \n",
    "        print(f\"| {exp_num} | {lr} | {gamma} | {batch} | {eps_start} | {eps_end} | {eps_decay} | {behavior} |\")\n",
    "    \n",
    "    if auto_fill:\n",
    "        print(\"\\nNOTE: Behaviors are generated from training metrics.\")\n",
    "        print(\"You can enhance these descriptions by:\")\n",
    "        print(\"1. Reviewing TensorBoard logs for detailed reward trends\")\n",
    "        print(\"2. Observing episode lengths and convergence patterns\")\n",
    "        print(\"3. Comparing with other experiments\")\n",
    "        print(\"4. Adding specific observations about stability and learning speed\")\n",
    "    else:\n",
    "        print(\"\\nINSTRUCTIONS:\")\n",
    "        print(\"1. Review TensorBoard logs for each experiment\")\n",
    "        print(\"2. Note the reward trends (increasing, stable, fluctuating)\")\n",
    "        print(\"3. Observe episode lengths over time\")\n",
    "        print(\"4. Check convergence speed (how quickly rewards improve)\")\n",
    "        print(\"5. Note any stability issues (erratic behavior)\")\n",
    "        print(\"6. Record final performance metrics\")\n",
    "        print(\"7. Fill in the 'Observed Behavior' column with your findings\")\n",
    "        print(\"8. Copy this table to your README.md file\")\n",
    "\n",
    "# Generate the table with behavior analysis\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\nGenerating hyperparameter table with behavior analysis...\\n\")\n",
    "    create_hyperparameter_table(results_df, auto_fill=True)\n",
    "    \n",
    "    # Also save detailed analysis to a file\n",
    "    print(\"DETAILED EXPERIMENT ANALYSIS\")\n",
    "    successful = results_df[results_df['status'] == 'success'].copy()\n",
    "    if len(successful) > 0 and 'best_mean_reward' in successful.columns:\n",
    "        valid_rewards = successful['best_mean_reward'].dropna()\n",
    "        if len(valid_rewards) > 0:\n",
    "            print(f\"\\nBest Performing Experiment: {successful.loc[valid_rewards.idxmax(), 'experiment']}\")\n",
    "            print(f\"  Reward: {valid_rewards.max():.2f}\")\n",
    "            print(f\"\\nWorst Performing Experiment: {successful.loc[valid_rewards.idxmin(), 'experiment']}\")\n",
    "            print(f\"  Reward: {valid_rewards.min():.2f}\")\n",
    "            print(f\"\\nAverage Reward Across All Experiments: {valid_rewards.mean():.2f}\")\n",
    "            print(f\"Standard Deviation: {valid_rewards.std():.2f}\")\n",
    "else:\n",
    "    print(\"Run experiments first to generate the table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for viewing TensorBoard\n",
    "print(\"To view training progress with TensorBoard, run in terminal:\")\n",
    "print(\"\\n tensorboard --logdir logs/\")\n",
    "print(\"\\nThen open http://localhost:6006 in your browser\")\n",
    "print(\"\\nOr use TensorBoard in Jupyter:\")\n",
    "print(\" %load_ext tensorboard\")\n",
    "print(\" %tensorboard --logdir logs/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load and Test Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model and test it\n",
    "def test_model(model_path, num_episodes=3, render=False):\n",
    "    \"\"\"Test a trained model\"\"\"\n",
    "    \n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = make_env(render_mode=\"human\" if render else None)\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model = DQN.load(model_path, env=env)\n",
    "        print(\" Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading model: {e}\")\n",
    "        env.close()\n",
    "        return\n",
    "    \n",
    "    # Set to greedy policy (no exploration)\n",
    "    model.exploration_rate = 0.0\n",
    "    \n",
    "    # Test episodes\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"\\nAverage reward over {num_episodes} episodes: {avg_reward:.2f}\")\n",
    "    \n",
    "    return avg_reward\n",
    "\n",
    "# Test the best model from first experiment (if available)\n",
    "if len(experiments) > 0:\n",
    "    test_model_path = f\"models/exp_{experiments[0]['num']}_CnnPolicy/best_model.zip\"\n",
    "    if os.path.exists(test_model_path):\n",
    "        print(\"Testing trained model...\")\n",
    "        test_model(test_model_path, num_episodes=3, render=False)\n",
    "    else:\n",
    "        print(f\"Model not found at {test_model_path}\")\n",
    "        print(\"Train a model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and save the best model as dqn_model.zip for use in play.py\n",
    "# This is REQUIRED for the assignment\n",
    "\n",
    "# Load results from CSV if available (works even after notebook restart)\n",
    "if os.path.exists('experiment_results.csv'):\n",
    "    try:\n",
    "        results_df = pd.read_csv('experiment_results.csv')\n",
    "        all_results = results_df.to_dict('records')\n",
    "        print(f\"Loaded {len(all_results)} experiment results from CSV\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results: {e}\")\n",
    "        all_results = []\n",
    "elif 'all_results' not in globals():\n",
    "    all_results = []\n",
    "\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    successful = results_df[results_df['status'] == 'success'].copy()\n",
    "    \n",
    "    if len(successful) > 0:\n",
    "        # Find the best performing experiment based on best_mean_reward\n",
    "        if 'best_mean_reward' in successful.columns:\n",
    "            valid_rewards = successful['best_mean_reward'].dropna()\n",
    "            if len(valid_rewards) > 0:\n",
    "                best_exp_idx = valid_rewards.idxmax()\n",
    "                best_exp = successful.loc[best_exp_idx]\n",
    "                best_exp_num = int(best_exp['experiment'])\n",
    "                \n",
    "                # Path to best model\n",
    "                best_model_path = f\"models/exp_{best_exp_num}_CnnPolicy/best_model.zip\"\n",
    "                final_model_path = f\"models/exp_{best_exp_num}_CnnPolicy/dqn_model.zip\"\n",
    "                \n",
    "                # Try best_model first, then final model\n",
    "                if os.path.exists(best_model_path):\n",
    "                    import shutil\n",
    "                    shutil.copy(best_model_path, \"dqn_model.zip\")\n",
    "                    print(f\" Best model copied from Experiment {best_exp_num}\")\n",
    "                    print(f\"  Source: {best_model_path}\")\n",
    "                    print(f\"  Destination: dqn_model.zip\")\n",
    "                    print(f\"  Best Reward: {best_exp['best_mean_reward']:.2f}\")\n",
    "                elif os.path.exists(final_model_path):\n",
    "                    import shutil\n",
    "                    shutil.copy(final_model_path, \"dqn_model.zip\")\n",
    "                    print(f\" Final model copied from Experiment {best_exp_num}\")\n",
    "                    print(f\"  Source: {final_model_path}\")\n",
    "                    print(f\"  Destination: dqn_model.zip\")\n",
    "                    print(f\"  Best Reward: {best_exp['best_mean_reward']:.2f}\")\n",
    "                else:\n",
    "                    print(f\" Model files not found for Experiment {best_exp_num}\")\n",
    "                    print(f\"  Expected: {best_model_path} or {final_model_path}\")\n",
    "                    print(f\"  Please check if training completed successfully.\")\n",
    "            else:\n",
    "                print(\"No reward data available. Using first successful experiment.\")\n",
    "                first_exp = successful.iloc[0]\n",
    "                first_exp_num = int(first_exp['experiment'])\n",
    "                model_path = f\"models/exp_{first_exp_num}_CnnPolicy/dqn_model.zip\"\n",
    "                if os.path.exists(model_path):\n",
    "                    import shutil\n",
    "                    shutil.copy(model_path, \"dqn_model.zip\")\n",
    "                    print(f\" Model copied from Experiment {first_exp_num}\")\n",
    "                else:\n",
    "                    print(f\" Model not found at {model_path}\")\n",
    "        else:\n",
    "            print(\"No reward data available. Using first successful experiment.\")\n",
    "            first_exp = successful.iloc[0]\n",
    "            first_exp_num = int(first_exp['experiment'])\n",
    "            model_path = f\"models/exp_{first_exp_num}_CnnPolicy/dqn_model.zip\"\n",
    "            if os.path.exists(model_path):\n",
    "                import shutil\n",
    "                shutil.copy(model_path, \"dqn_model.zip\")\n",
    "                print(f\" Model copied from Experiment {first_exp_num}\")\n",
    "            else:\n",
    "                print(f\" Model not found at {model_path}\")\n",
    "        \n",
    "        # Verify the file exists\n",
    "        if os.path.exists(\"dqn_model.zip\"):\n",
    "            file_size = os.path.getsize(\"dqn_model.zip\") / (1024 * 1024)  # MB\n",
    "            print(f\"\\n dqn_model.zip is ready for play.py\")\n",
    "            print(f\"  File size: {file_size:.2f} MB\")\n",
    "        else:\n",
    "            print(\"\\n dqn_model.zip was not created. Please check model paths.\")\n",
    "    else:\n",
    "        print(\"No successful experiments found. Run experiments first.\")\n",
    "else:\n",
    "    print(\"No results available. Run experiments first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
